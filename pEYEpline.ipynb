{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b21695-ad36-4687-8cd8-e2b4ee5237c9",
   "metadata": {},
   "source": [
    "# pEYEpline: Preprocessing of the MultiplEYE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ff573-fae4-4625-82fc-d528d7feab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd22311-66cc-46c2-a390-48064bb10aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83dd2108-0c40-4346-9236-8ac4a041dcc5",
   "metadata": {},
   "source": [
    "Files that go to the MultiplEyeStore repository:\n",
    "\n",
    "\n",
    "- **Raw recordings:** Raw files without any formatting applied (in the original encoding). --> to be decided if these data should really be included (con: storage; pro: stage 0 is good to have, some messages in the original files will be lost in the first step of preprocessing.) **Filename** must contain: participantId, stimulusId, TrialId\n",
    "  \n",
    "- **Raw data:** one csv file per stimulus text and per reader containing the following columns: ScreenId (x,y) screen location in pixels, gaze event (fixation, saccade, NaN, blink), optionally pupil size. **Filename** must contain: participantId, stimulusId, TrialId\n",
    "\n",
    "- **Fixation data:** one csv file per stimulus text and per reader containing one fixation per line with the following columns: ScreenId, onset-time, offset-time, (x,y) screen location in pixels (mean and std), duration, etc. **Filename** must contain: participantId, stimulusId, TrialId\n",
    "\n",
    "- **Saccade data:** one csv file per stimulus text and per reader containing one saccade per line with the following columns: screenId, onset-time, offset-time, start (x,y) screen location in pixels, end (x,y) screen location in pixels, duration, amplitude in deg of visual angle, amplitude in chars, mean velocity, peak velocity. **Filename** must contain: participantId, stimulusId, TrialId\n",
    "\n",
    "- **Interest area files** word- and char-based interest area files (can be merged with data when loading it), contain line information. **Filename** must contain: participantId, stimulusId, TrialId\n",
    "\n",
    "- **Reading measures files**: one csv file per stimulus text and per reader containing reading measures, aois, screenIds. **Filename** must contain: participantId, stimulusId, TrialId\n",
    "\n",
    "- **Data quality reports**:\n",
    "  - **Trial-level data quality reports:** json; one file per stimulus text per participant. **Filename** must contain: ParticipantId, StimulusId, TrialId\n",
    "  - **Session-level data quality reports:** json; One file per session (participant). Contains data quality measures aggregated for all data from one sesseion.  **Filename** must contain: participantId, \n",
    "  - **Dataset-level data quality reports** json; One file per dataset\n",
    "\n",
    "- **Response accuracies and text difficulty and familiarity ratings:** one csv or json file for each stimulus containing itemId, recorded response (pressed key), response type {target, distractor a, distractor b, distractor c}, response accuracies and latencies for all questions (=items) and and the text difficulty and familiarity rating for this text. **Filename** must contain: participantId, stimulusId\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f20cd8-8920-449a-a5c9-aaaa451b88a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODOs:\n",
    "# generate data quality report for each session asap after the session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d7ccc-6432-4bc6-985d-e42862536673",
   "metadata": {},
   "source": [
    "## Stimulus texts preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d90d8a-a969-435f-b4f1-f5b440ed661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set set-up-specific variable values; default values are set for DiLi lab, ZH\n",
    "eyetracker = \"eyelink\"\n",
    "# TODO add all relevant set-up specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38973cd4-2ed4-4c1b-8b94-07378034ef11",
   "metadata": {},
   "source": [
    "*Terminology*\n",
    "- output = {preprocessing, filter-criteria, repository}\n",
    "- raw_recordings: eyetracking recording files generated by the eyetracker converted to *human-readable pure text format* (ascii)\n",
    "- raw_files: eyetracking raw recordings (csv format) in device-unspecific format containing times stamps, and screen coordinates in pixels, and, optionally, pupil size and unit\n",
    "- quality_report_raw_asc: file containing data quality measures extracted from the raw_recordings asc files (csv); eye-tracker-unspecific format, eyetracker-specific contents (missing values for some devices); one file per participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08dac7-3f3e-4d1e-b02f-b59773eb3a34",
   "metadata": {},
   "source": [
    "## Eye movements preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836e064-5023-47f3-94d0-121ce60ebeb0",
   "metadata": {},
   "source": [
    "Compute different representations of eye movements (raw samples, gaze event data, reading measures) and add relevant information (aois) and the various identifiers (textiId, screenId, trialId).\n",
    "\n",
    "**Generate one file per participant per text for all stages of preprocessing (raw, events, reading measures).**\n",
    "\n",
    "**Identifiers:**\n",
    "Encode textId, participantId, and trialId only in the filename;\n",
    "Add screenId to the data\n",
    "\n",
    "**Interest areas**\n",
    "For the raw and the evant data, aois (char-based and word-based) will be stored as separate files that need to be merged with the eye movement data via the gaze/aoi screen coordinates when loading the data. \n",
    "\n",
    "**Note:** implement this when adding multipleye to the pymovements library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2fe12-257d-401c-a68c-bd5091198f14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Processing of eye-tracker-specific raw recording files (e.g., edf files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59033c40-1354-45bd-8a91-f64357fc601a",
   "metadata": {},
   "source": [
    "#### Eyetracker-specific recording files to human-readable format\n",
    "- eyetracker-specific step\n",
    "- input is eyetracker specific, output is still eyetracker-specific\n",
    "- only applicable if original eyetracking recording files are not human readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b56fd-2d9c-45ab-ae45-2f6e01de93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eyetracker == \"eyelink\"\n",
    "    # load edf files\n",
    "    # apply edf2asc \n",
    "    # input: all edf data files\n",
    "    # output: \n",
    "        # name: raw_recordings\n",
    "        # format: asc (eyetracker specific contents)\n",
    "        # goes to: preprocessing\n",
    "elif eyetracker == \"tobii\"\n",
    "    #load raw_files\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d806910-719b-46c9-9463-df46cae49f2b",
   "metadata": {},
   "source": [
    "### Processing of eyetracker-specific human-readable raw recording files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c254092-1829-4d07-aee6-615668eb117f",
   "metadata": {},
   "source": [
    "- Eyetracker-specific input\n",
    "- Output should be eyetracker-unspecific in format, but will contain missing values in some places depending on the device\n",
    "- Extract information that are directly written as meta-information into the recording file (information about calibration scores etc)\n",
    "- Only stimulus-independent metrics\n",
    "- No metrics that need to be calculated from the data samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ceef7-6c22-4a83-8f58-351882983d65",
   "metadata": {},
   "source": [
    "#### Blink extraction \n",
    "\n",
    "- only applicable if eyetracker provides blinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20f6c0-eab3-47ed-a3fb-f2ee45713139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blink detection: \n",
    "# If provided by the eyetracker: extract blinks (times stamp and duration) and write to csv\n",
    "\n",
    "# Input: raw_recordings\n",
    "# Output:\n",
    "    # name: blinks_eyetracker\n",
    "    # format: csv \n",
    "    # goes to: preprocessing (data quality reports, gaze events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3cf88-67ab-456e-aada-755c1d58d9bb",
   "metadata": {},
   "source": [
    "#### Extract data quality information from eyetracker-specific recording files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9aa4a6-4cc6-4ce7-91da-e6b2daff5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data quality report that contains information that the eyetracker writes as meta-data into the recording\n",
    "# TODO decide about the exact metrics\n",
    "# Preliminary list of values/information to extract \n",
    "# - Is the session complete? (all stimuli being completed and all questions answered); of not: how much is missing? (e.g., provide proportion completed in terms of screens and in terms of texts)\n",
    "# - Have all comprehension questions been answered? (if this information is available in raw recordings)\n",
    "# - Was calibration and validation performed at the beginning of the experiment? (scores?)\n",
    "# - Has validation performed before each text? extract validation scores before each text (if validation was followed by a calibration and then a second validation, use the scores from the second validation)\n",
    "# - Was a validation check  performed at the end of the recording? Extract validation scores. \n",
    "# - calibration scores, validation scores; when/how many calibrations were performed?\n",
    "# - when/to what extent has drift correction been performed? (timestamp, before which trial/item id? Was the drift corrected or only checked?\n",
    "# - What (if any) filter was applied for data recording?\n",
    "# - if blinks have been extracted (see above), compute proportion/frequency of blinks, and some measure reflecting their mean and std duration (or median)\n",
    "\n",
    "# Input: raw_recordings\n",
    "# Output:\n",
    "    # name: quality_report_raw_asc\n",
    "    # format: csv or json\n",
    "    # goes to: preprocessing: session-level data quality reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126218c-2ea3-4a7f-b37d-bc84dd3ba824",
   "metadata": {},
   "source": [
    "#### Parsing of eyetracker-specific raw recording files to consistent eyetracker-unspecific csv files containing the raw samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc45daa-5499-4d9c-9f63-0a24ae73cece",
   "metadata": {},
   "source": [
    "- Eytracker-specific input/code, output eyetracker-unspecific\n",
    "- Apply inclusion criteria: a given participant needs to have completed reading at least one entire text (practice texts do not count) and have answered the comprehension questions for that text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09f742-578c-429c-8bb9-e48987939de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process raw recording files to csv with \n",
    "\n",
    "# Generate one file for each participant with the following columns: trialID, stimulusId, screenId, timestamp in ms, x-gaze coordinate in screen pixels, y-gaze coordinate in screen pixels, optional: pupil size, pupil size measurement unit (diameter, area...)\n",
    "# Ensure that the same coordinate system is used across devices/datasets\n",
    "# make sure to split data by stimulusId (=text) and screenid\n",
    "# Apply inclusion criterion: remove participants who have not completed at least one entire text plus the corresponding comprehension questions\n",
    "# merge multiple eyetracking files from one participant (only applicable if experiment was aborted and re-started)\n",
    "# handle any other inconsistencies (wrong participant IDs (caution: the id is in many files), aborted trials, missing data,....\n",
    "\n",
    "# Arguments: eyetracking device (format of raw_recordings)\n",
    "# Input: raw_recordings\n",
    "# Output: \n",
    "    # name: raw_files\n",
    "    # format: csv (consistent columns across devices; trialId, stimuulusId, screenId, x,y-screen px coords, optional column for pupil size); one file for each participant\n",
    "    # goes to: preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c986b32a-a56f-45d4-a8a3-0d07247eaaf0",
   "metadata": {},
   "source": [
    "### Processing of raw samples (csv format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92760168-d77f-4982-9d5c-bc728f3b1400",
   "metadata": {},
   "source": [
    "#### Generate data quality measures from raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d05607-85b2-4676-8ee1-2afbaaecff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the following measures (preliminary list) from the raw eyetracking data: \n",
    "\n",
    "# duration of the recording (for each text or for each screen and total duration); \n",
    "# proportion of data loss\n",
    "\n",
    "# Input: raw_files \n",
    "# Output: \n",
    "    # name: quality_report_raw\n",
    "    # format: csv or json\n",
    "    # goes to: repository, filter_criteria (possibly after merging with quality measures computed at the other stages of preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8b2d6-cebe-4c3f-8941-e4db03d3bf55",
   "metadata": {},
   "source": [
    "#### Gaze event detection and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1cb11-e7f6-4543-90be-bd9eb2f752ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaze even detection \n",
    "# Compute gaze events and add them as additional column to the raw samples (saccade, fixation, blink, artifact/corrupt measurement)\n",
    "# Apply artifact detection, blink detection, saccade/fixation detection\n",
    "\n",
    "# Input: raw_files \n",
    "# Output: \n",
    "    # name: gave_event_files\n",
    "    # format: csv\n",
    "    # goes to: preprocessing, repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef56d3-6a87-4bc7-809b-20743582fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fixation files\n",
    "# From raw samples classified as fixations, compute fixation features:\n",
    "\n",
    "# Preliminary list of fixation features:\n",
    "# start timestamp\n",
    "# end timestamp\n",
    "# duration\n",
    "# standard deviation\n",
    "# location (mean)\n",
    "\n",
    "# Input: gaze_event_files\n",
    "# Output:\n",
    "    # name: fixation_files\n",
    "    # format: csv files; one fixation file and one saccade file per participant and text)\n",
    "    # goes to: preprocessing for adding aoi infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8ff7-c0ca-4dfb-be8b-b96224dc0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute saccade files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c916f5b-94c2-46ef-83fa-fbde560e4308",
   "metadata": {},
   "source": [
    "### Processing of raw samples (csv format) and aoi files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820eeaf-25f1-4811-8f96-52b5815431a9",
   "metadata": {},
   "source": [
    "Input is raw samples and aoi files (char-based or word-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f52783-d9f4-448f-896a-d502c33763b9",
   "metadata": {},
   "source": [
    "#### Add aoi information to raw samples\n",
    "- To be decided: Shall we share these files on the repository?\n",
    "    - Pro: maybe useful for some users who want to work on the raw data plus aoi info\n",
    "    - Con: Take a lot of space; these data can be easily generated by the user of MulitplEYE\n",
    "- (Potential) use cases of these data: plotting of raw data and aoi\n",
    "- Generation of Data quality report (next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097b167-feab-48b9-81c6-44200ee9100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge aoi files with raw data (aoi as additional columns and NaNs)\n",
    "# Input: char-based or word-based aoi's\n",
    "# Output: \n",
    "    # name: raw_files_aoi\n",
    "    # format: csv\n",
    "    # goes to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f3c46-1f2e-4d45-b242-ad71e0c3f364",
   "metadata": {},
   "source": [
    "#### Generate stimulus-dependent quality measures from raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4cf8e-d709-479f-9134-839dc8663f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93cb2ad-732e-404f-b59c-0f60b92e092d",
   "metadata": {},
   "source": [
    "### Add aois to fixation files\n",
    "Input: fixation files, word-based and char-based aoi files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d40a22-58bf-4d76-9e15-af04b34f34c5",
   "metadata": {},
   "source": [
    "## Write trial-level data quality reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89102141-d214-4a15-9de6-3968ad49aea9",
   "metadata": {},
   "source": [
    "## Write session-level data quality reports\n",
    "Combine all session-level data quality reports that have been generated at the different steps of the pipeline into a single report (one file per session (=reader)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad72e70-ef22-4958-a231-a2055f102dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all readers combine quality reports from all texts into a single session (=reader)-level quality report\n",
    "\n",
    "# Inputs: \n",
    "# quality_report_raw_asc, quality_report_raw, TODO\n",
    "# Output: \n",
    "    # name: session_level_quality_reports (one file per reader)\n",
    "    # format: json\n",
    "    # goes to: repository, filter-criteria, preprocessing (dataset-level data quality reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a80c8-b345-4fb7-992a-6d4883186eb1",
   "metadata": {},
   "source": [
    "## Generate dataset-level data quality reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce6ee9-7ac6-4258-8195-17c3941220d3",
   "metadata": {},
   "source": [
    "### Compute dataset-level data quality information from session-level data quality reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba678e-a6a3-46f0-8460-6cb8124d1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the session-level data quality measures to the dataset level\n",
    "# Input: session_level_quality_reports (one file per reader)\n",
    "# Output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98f3506-f7d9-4157-bc6f-2bc42c1552ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce7d9f0-d89e-4033-8e2b-c89616f8891f",
   "metadata": {},
   "source": [
    "### Get dataset-level data quality information from meta-data documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53eb815-0463-4641-95ce-2d460381c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read meta-data documentation, deviation form etc. TODO Which files \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da9357-5ea4-464f-9809-f6bf2058e55d",
   "metadata": {},
   "source": [
    "### Write dataset-level data quality report"
   ]
  },
  {
   "cell_type": "raw",
   "id": "292e0b11-e388-4cb3-8851-62ca2b65919e",
   "metadata": {},
   "source": [
    "Notes: \n",
    "Things that could be done (low priority): \n",
    "- Data quality reports for eye movements on comprehension questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8372a-6d24-4716-a992-32c98a8027b0",
   "metadata": {},
   "source": [
    "## Comprehension questions and difficulty/familiarity rating response processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611e9b2b-7a58-4b29-ab90-0d701b227b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the participant's response (pressed key) and target answer, compute response accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5bafa-032a-48df-8e06-2d3a999bbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each participant and each text, write file with response behavior and text difficulty and familiarity ratings:**\n",
    "# one csv or json file for each stimulus containing itemId, recorded response (pressed key), response type {target, distractor a, distractor b, distractor c}, response accuracies and latencies for all questions (=items) and and the text difficulty and familiarity rating for this text. **Filename** must contain: participantId, stimulusId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8595e-b06f-4634-87a6-02c2053fe318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
